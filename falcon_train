任务：从卫星图像中分割出所有道路


data  



  {
    "conversations": [
      {"content": "<image>\nPIX_SEG\nSegment out road in the image."}, // 问题/指令
      {"content": "<poly><465><221><443>...</poly>"} // 答案/坐标
    ],
    "images": ["path/to/image.png"] or ["path/to/before.png", "path/to/after.png"],
    "crop": [x1, y1, x2, y2] // 可选，裁剪坐标
  }



原始one-hot: [vocab_size]
embedding后: [hidden_size]  (例如768或512
Token ID (1个数) -> Embedding向量 (768维)
[单个数字] -> [0.1, -0.3, 0.5, ..., 0.2]

文本 -> 分词(Tokenization) -> Token ID -> Embedding

1.输入

输入："Segment out road in the image"
↓
分词结果：["Segment", "out", "road", "in", "the", "image"]


2.转化为ID
"Segment" -> 245
"out" -> 678
"road" -> 892
...

3.转化为向量，向量映射：
245 -> [0.1, -0.3, 0.5, ..., 0.2]  # 768维向量
678 -> [-0.2, 0.4, 0.1, ..., 0.3]  # 768维向量


相似词的向量距离较近：
road ≈ street ≈ avenue
[0.1, 0.2, ...] ≈ [0.11, 0.19, ...] ≈ [0.12, 0.21, ...]
...



one-hot
我你他
我，你，他

one hot 编码
1 0 0 
0 1 0 
0 0 1
词表：目前有多少词，这时候构成的向量就有多少个维度
但是会有维度灾难的问题




- 原始One-hot：V维（词表大小）
- 嵌入后：d_model维（通常512或768）
（B,P,D）B:批次大小 P ：一个图像被分成了多少个PATCH ,D模型的嵌入维度（这个就是解决维度灾难的方式）

图像输入：

一张卫星图像（例如512×512像素）
图像经预处理后被分割成196个14×14像素的图像块(patches)
每个图像块被线性投影成768维的特征向量
添加位置编码后形成序列：[v₁, v₂, ..., v₁₉₆]，其中每个vᵢ是768维向量


文本输入：
指令："PIX_SEG\nSegment out road in the image."
经过分词器处理后变成tokens：["PIX", "SEG", "\n", "Segment", "out", "road", "in", "the", "image", "."]
每个token被嵌入为768维向量，形成序列：[t₁, t₂, ..., t₁₀]


2. 多模态交互阶段
特征融合：
图像特征序列和文本特征序列被送入多模态Transformer
自注意力层：每个特征与序列中所有其他特征交互
交叉注意力层：文本特征与图像特征交互


空间定位：
"road"对应的token向量t₆会与表示道路的图像块(如v₃₀, v₄₅, v₆₇等)产生高注意力权重
模型通过注意力机制识别出图像中道路区域的位置


3. 生成阶段
模型开始以自回归方式生成标记序列
首先生成<poly>标记
然后预测第一个坐标点<465><221>
接着预测下一个坐标点<443><279>
依此类推，逐步预测道路轮廓的所有坐标点
最后生成</poly>标记结束多边形定义

<poly><465><221><443><279><476><298><501><267><509><221><490><202></poly>


4. 视觉化解释
在模型内部，注意力权重可以可视化为热力图，显示模型关注的区域：
当生成第一个坐标点<465><221>时，模型注意力主要集中在图像中道路的起始位置
随着序列生成的进行，注意力会沿着道路轮廓移动
模型的注意力机制形成了从视觉特征到坐标序列的"桥梁"
